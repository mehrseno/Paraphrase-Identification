{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrseno/Paraphrase-Identification/blob/main/Paraphrase_Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJuLS2At_nlB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsVApwzXSloS"
      },
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install numpy==1.20.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLNLeW1tJTAF"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# train https://drive.google.com/file/d/17eLq5Ng5yfbX9tLq2tdiOdscO0ea8xK5/view?usp=sharing\n",
        "# valid https://drive.google.com/file/d/1Y-68UagW14hwJXNyaR2HnML5jEwvjQlj/view?usp=sharing\n",
        "# test https://drive.google.com/file/d/1P_HLLvGq15gsgDl5P6QYrHkTlkcytMdr/view?usp=sharing\n",
        "\n",
        "!gdown --id 17eLq5Ng5yfbX9tLq2tdiOdscO0ea8xK5\n",
        "!gdown --id 1Y-68UagW14hwJXNyaR2HnML5jEwvjQlj\n",
        "!gdown --id 1P_HLLvGq15gsgDl5P6QYrHkTlkcytMdr\n",
        "\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "valid = pd.read_csv(\"valid.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uEBuvAdiDRe"
      },
      "source": [
        "# remove punctuation marks, non-alphabetic tokens, and lowercase the tokens\n",
        "def clean_data(data):\n",
        "\n",
        "  tokenized_data = []\n",
        "\n",
        "  for d in data:\n",
        "    tokens = d.split()\n",
        "    translation_table = str.maketrans('', '', \"\\\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~،؟!❊#$٪^&*)(ـ+=-؛:{}\")\n",
        "    tokens = [w.translate(translation_table) for w in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokenized_data.append(tokens)\n",
        "\n",
        "  return tokenized_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or_EerQGMsWv",
        "outputId": "e91f49da-5b61-483d-8d4d-d04e3d274187"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "length = []\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "  length.append(len(row.sentence1.split(\" \")))\n",
        "  length.append(len(row.sentence2.split(\" \")))\n",
        "\n",
        "print(np.mean(length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18.972063232488416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaMyvFk8hXlv"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "\n",
        "# clean data\n",
        "cleaned_train_data = clean_data(train.sentence1.append(train.sentence2))\n",
        "cleaned_valid_data = clean_data(valid.sentence1.append(valid.sentence2))\n",
        "cleaned_test_data = clean_data(test.sentence1.append(test.sentence2))\n",
        "\n",
        "# create the tokenizer and fit it on the input text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(cleaned_train_data)\n",
        "\n",
        "# tokenize the input text into encoded numbers\n",
        "train_sequences = tokenizer.texts_to_sequences(cleaned_train_data)\n",
        "valid_sequences = tokenizer.texts_to_sequences(cleaned_valid_data)\n",
        "test_sequences = tokenizer.texts_to_sequences(cleaned_test_data)\n",
        "\n",
        "# pad the input text \n",
        "train_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
        "valid_sequences = pad_sequences(valid_sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
        "\n",
        "# separate sentence1 and sentence2\n",
        "train_sent1 = train_sequences[:len(train)]\n",
        "train_sent2 = train_sequences[len(train): len(train) * 2]\n",
        "valid_sent1 = valid_sequences[:len(valid)]\n",
        "valid_sent2 = valid_sequences[len(valid): len(valid) * 2]\n",
        "test_sent1 = test_sequences[:len(test)]\n",
        "test_sent2 = test_sequences[len(test): len(test) * 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm-mvb-Y0N0W"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "VOCAB_SIZE = len(word_index) + 1\n",
        "EMB_SIZE = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WabwVIwfZTGj"
      },
      "source": [
        "# train word2vec \n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "train_words = clean_data(train.sentence1.append(train.sentence2))\n",
        "\n",
        "w2v_model = Word2Vec(train_words, min_count=1, vector_size=EMB_SIZE, window=3, sg=1)\n",
        "\n",
        "words = list(w2v_model.wv.key_to_index.keys())\n",
        "embeddings = w2v_model.wv.vectors\n",
        "\n",
        "lines = []\n",
        "for i in range(len(words)):\n",
        "    line = words[i]\n",
        "    for j in range(len(embeddings[i])):\n",
        "        line += ' '\n",
        "        line += str(embeddings[i][j])\n",
        "    lines.append(line)\n",
        "\n",
        "with open('drive/MyDrive/MCI/HW5/MRPC.' + str(EMB_SIZE) + '.vec', 'w') as f:\n",
        "    for l in lines:\n",
        "        f.write(\"%s\\n\" % l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpzbILawYKE5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open('drive/MyDrive/MCI/HW5/MRPC.' + str(EMB_SIZE) + '.vec')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.random.random((VOCAB_SIZE, EMB_SIZE))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8P_TSJV3uZh"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import optimizers, losses\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.metrics import *\n",
        "# from keras_self_attention import SeqSelfAttention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRdEg-7nGFx8"
      },
      "source": [
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.W = self.add_weight(name=\"attention_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"attention_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")      \n",
        "\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
        "        at = K.softmax(et)\n",
        "        at = K.expand_dims(at, axis=-1)\n",
        "        output = x * at\n",
        "        # return K.sum(output, axis=1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJPrn0SKVGLd"
      },
      "source": [
        "# define model\n",
        "def get_model(VOCAB_SIZE, EMB_SIZE, MAX_SEQ_LENGTH, embedding_matrix):\n",
        "\n",
        "  # define layers\n",
        "  embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], mask_zero=True, name='EmbeddingLayer')\n",
        "  bilstm_layer = Bidirectional(LSTM(EMB_SIZE, dropout=0.2, activation=\"tanh\", return_sequences=True, recurrent_dropout=0.5))\n",
        "  att_layer = Attention()\n",
        "\n",
        "\n",
        "  # sentence1\n",
        "  sent1_input = Input(shape=(MAX_SEQ_LENGTH,), name='Sentence1')\n",
        "  sent1_emb = embedding_layer(sent1_input)\n",
        "  sent1_bilstm = bilstm_layer(sent1_emb)\n",
        "  sent1_att = att_layer(sent1_bilstm)\n",
        "\n",
        "\n",
        "  # sentence2\n",
        "  sent2_input = Input(shape=(MAX_SEQ_LENGTH,), name='Sentence2')\n",
        "  sent2_emb = embedding_layer(sent2_input)\n",
        "  sent2_bilstm = bilstm_layer(sent2_emb)\n",
        "  sent2_att = att_layer(sent2_bilstm)\n",
        "  \n",
        " \n",
        "  concat_layer = Concatenate(name='concatenate', axis=1)([sent1_att, sent2_att])\n",
        "  flatten = Flatten()(concat_layer)\n",
        "\n",
        "  drop_layer1 = Dropout(0.2)(flatten)\n",
        "  dense_layer1 = Dense(512, activation='relu', name='DenseLayer1')(drop_layer1)\n",
        "  dense_layer2 = Dense(128, activation='relu', name='DenseLayer2')(drop_layer1)\n",
        "  dense_layer3 = Dense(64, activation='relu', name='DenseLayer3')(dense_layer2)\n",
        "  dense_layer4 = Dense(32, activation='relu', name='DenseLayer4')(dense_layer3)\n",
        "  drop_layer2 = Dropout(0.2)(dense_layer4)\n",
        "\n",
        "  out = Dense(2, activation='softmax', name='OutputLayer')(drop_layer2)\n",
        "\n",
        "  model = Model(inputs=[sent1_input, sent2_input], outputs=out)\n",
        "\n",
        "  optimizer = optimizers.Adam(learning_rate=2e-4)\n",
        "  loss = losses.binary_crossentropy\n",
        "\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', Recall(), Precision()])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLAvbcs45lmr"
      },
      "source": [
        "model = get_model(VOCAB_SIZE, EMB_SIZE, MAX_SEQ_LENGTH, embedding_matrix)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uMzmL9WAM8g"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = np.array(to_categorical(train.label), dtype=int)\n",
        "y_valid = np.array(to_categorical(valid.label), dtype=int)\n",
        "y_test = np.array(to_categorical(test.label), dtype=int)\n",
        "\n",
        "max_epoch = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgH2qvNyukEz"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class VisualiseAttentionMap(Callback):\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            concatenate_layer_out = Model(inputs=model.input, outputs=model.get_layer('concatenate').output)\n",
        "            outputs = concatenate_layer_out.predict([test_sent1, test_sent2])\n",
        "\n",
        "            plt.imshow(outputs[0][:MAX_SEQ_LENGTH], cmap='Blues')\n",
        "\n",
        "            iteration_no = str(epoch).zfill(3)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Iteration {iteration_no} / {max_epoch} Sentence1')\n",
        "            plt.savefig('attention1.png')\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "            plt.clf()\n",
        "\n",
        "            plt.imshow(outputs[0][MAX_SEQ_LENGTH:], cmap='hot')\n",
        "\n",
        "            iteration_no = str(epoch).zfill(3)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Iteration {iteration_no} / {max_epoch} Sentence2')\n",
        "            plt.savefig('attention2.png')\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "            plt.clf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR9JhdFEOhjX"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/MCI/HW5/bilstm.ckpt'\n",
        "\n",
        "cp_callback = [ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', mode = 'max', save_best_only=True), EarlyStopping(patience=10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1Y0J51P4wWu"
      },
      "source": [
        "history = model.fit(\n",
        "    [train_sent1, train_sent2],\n",
        "    y_train,\n",
        "    validation_data = ([valid_sent1, valid_sent2], y_valid),\n",
        "    epochs=max_epoch,\n",
        "    verbose=1,\n",
        "    batch_size=128,\n",
        "    callbacks=[VisualiseAttentionMap(), cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxGVkNHz5Oqj"
      },
      "source": [
        "plt.style.use('ggplot')\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDmqNgEO5PvK"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdEp5GstLxLt"
      },
      "source": [
        "predictions = model.predict([test_sent1, test_sent2])\n",
        "\n",
        "pred_labels = [np.argmax(predictions[i]) for i in range(len(y_test))]\n",
        "\n",
        "y_test = test.label.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seMh9d2EA4CC"
      },
      "source": [
        "from sklearn.metrics import *\n",
        "\n",
        "print(classification_report(y_test, pred_labels, target_names=['Class 0', 'Class 1']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaMnFkzWL0cg"
      },
      "source": [
        "print(\"Test Accuracy:\", accuracy_score(y_test, pred_labels)) \n",
        "print('Test Precision (micro):', precision_score(y_test, pred_labels, average='micro'))\n",
        "print('Test Recall (micro):', recall_score(y_test, pred_labels, average='micro'))\n",
        "print('Test F1 Score (micro):', f1_score(y_test, pred_labels, average='micro'))\n",
        "print('Test Precision (macro):', precision_score(y_test, pred_labels, average='macro'))\n",
        "print('Test Recall (macro):', recall_score(y_test, pred_labels, average='macro'))\n",
        "print('Test F1 Score (macro):', f1_score(y_test, pred_labels, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
